

<!DOCTYPE html>

<html>

    <head>
        <meta charset="UTF-8">
        <title>TFG</title>

        <link rel="stylesheet" href="https://waloncab.github.io/s.ln_blog/assets/css/body.css">
        <link rel="stylesheet" href="./style.css">
        <link rel="stylesheet" href="./codehilite.css">

        

<script type="module">
  import mermaid from 'https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script>


        
    </head> 

    <body>

        <div class="content">

            <ul><li><a href="#TFG">TFG</a></li><ul><li><a href="#resumen">resumen</a></li><li><a href="#0-intro">0-intro</a></li><ul><li><a href="#contexto">contexto</a></li><li><a href="#previos">previos</a></li><li><a href="#justificación">justificación</a></li></ul><li><a href="#0-plan">0-plan</a></li><ul><li><a href="#alcance">alcance</a></li><li><a href="#metodología">metodología</a></li><li><a href="#EDT">EDT</a></li><li><a href="#entregables">entregables</a></li><li><a href="#tiempos">tiempos</a></li><li><a href="#gantt">gantt</a></li><li><a href="#rrhh">rrhh</a></li><li><a href="#comunicaciones">comunicaciones</a></li><li><a href="#riegos">riegos</a></li></ul><li><a href="#0-analisis">0-analisis</a></li><ul><li><a href="#AGRAI-data">AGRAI-data</a></li><li><a href="#ideas-previas">ideas-previas</a></li><li><a href="#automatizacion">automatizacion</a></li><li><a href="#tecnologías">tecnologías</a></li><li><a href="#requisitos">requisitos</a></li></ul><li><a href="#0-infraestructura">0-infraestructura</a></li><ul><li><a href="#infraestructure">infraestructure</a></li><li><a href="#entorno">entorno</a></li><li><a href="#UNIX">UNIX</a></li><li><a href="#control-versiones">control-versiones</a></li></ul><li><a href="#0-diseño">0-diseño</a></li><ul><li><a href="#componentes">componentes</a></li><li><a href="#data-load">data-load</a></li><li><a href="#modelo-BD">modelo-BD</a></li><li><a href="#services">services</a></li><li><a href="#despliegue">despliegue</a></li></ul><li><a href="#0-implementacion">0-implementacion</a></li><ul><li><a href="#previos-pipe">previos-pipe</a></li><li><a href="#commands-CLI">commands-CLI</a></li><li><a href="#model-view">model-view</a></li><li><a href="#modelo-producción">modelo-producción</a></li></ul><li><a href="#0-seguimiento">0-seguimiento</a></li><ul><li><a href="#seguimiento">seguimiento</a></li><li><a href="#memoria">memoria</a></li><li><a href="#lecciones">lecciones</a></li></ul><li><a href="#0-conclusiones">0-conclusiones</a></li><ul><li><a href="#conclusiones">conclusiones</a></li><li><a href="#bibliografía">bibliografía</a></li></ul></ul></ul>

            <div id=resumen><h2>2 resumen</h2><p>Pipeline de Datos para una Aplicación de datos Agroalimentarios.</p>
<p>Resumen: El trabajo consistirá en la reestructuración de una aplicación de gestión de datos agroalimentarios con el objetivo de soportar la gestión y el mantenimiento de los datos de diferentes clientes. Estos datos, actualmente, se actualizan e incrementan de forma periódica con un añadido de trabajo manual que se puede optimizar mediante técnicas de integración y despliegue continuo.</p></div><div id=contexto><h3>3 contexto</h3><p>El propósito de este proyecto es mejorar el flujo de trabajo del equipo de "SpectralGeo" mediante la automatización del proceso de recogida y procesado de datos agronómicos que se utilizan en la creación de modelos de inteligencia artificial.</p>
<p>AGRAI es una aplicación para la gestión de cultivos que permite al agricultor monitorizar el estado de sus parcelas. El estado actual de la aplicación se centra el despliegue de datos agronómicos y vegetativos georeferenciados a través de en una interfaz web. Los usuarios de la aplicación, normalmente agricultores o cooperativas, pueden consultar el estado de su parcelario junto con algunas predicciones. El acceso a dicha aplicación puede darse desde equipo de sobremesa o un dispositivo móvil, aunque es este último lo que parece que se utiliza más. </p>
<p>
<figure><img src="figures/visor_GIS_det.png" /><figcaption>caption</figcaption>
</figure>
</p>
<p>Actualmente, cómo se procesa la información que utiliza nuestra aplicación es un proceso tedioso para el equipo. En este proceso, diferentes miembros trabajan con tecnologías distintas sobre datos duplicados provenientes de fuentes comunes. Aunque se realiza una planificación y coordinación de los proyectos, se pierde bastante tiempo en la transformación de los datos que cada miembro del equipo necesita para llevar a cabo su labor.</p>
<p>El objetivo de este Trabajo de Fin de Grado (TFG) es mejorar y automatizar el flujo de trabajo del equipo, convirtiendo la aplicación AGRAI en una herramienta robusta que manipule un único repositorio de datos al cual el resto del equipo pueda acceder, utilizar de forma segura y lo más importante, sin duplicar y romper la integridad de estos datos.</p>
<p>Entiendo que es un proyecto ambicioso debido a que no solo es importante el conocimiento técnico, sino que serán necesarios cambios en la forma de trabajo del equipo, además de la confianza de cada miembro por la nueva forma de trabajo que se desea implementar. </p></div><div id=previos><h3>3 previos</h3><p>SpectralGeo se ha especializado en el uso de nuevas tecnologías para sectores como el de la agricultura o el reciclaje, centrándose en proyectos con carácter reivindicativo por la sostenible medioambiental. La relación con clientes como Ecoembes o el desarrollo de software para la gestión sostenible de cultivos lo demuestran.</p>
<p>Durante las prácticas realizas en la empresa identifiqué que era totalmente necesario dotar de una arquitectura robusta a la aplicación sobre la que trabajaba gran parte del equipo. Además de adaptarme a las tecnologías necesarias para la ciencia de datos actual al que todo el mundo se está sumando, comencé con la implementación de un modelo sobre el que la aplicación AGRAI pudiese escalarse posteriormente.</p>
<p>La aplicación ya está creada, el cliente obtiene los resultados que espera cuando consulta el estado de su parcelario en la interfaz de la aplicación. Como equipo nos organizamos para que la información y los modelos predictivos lleguen al cliente a través de esta, pero son varios los puntos en los que trabajamos en exceso para finalmente mostrar una "predicción" al cliente. Serán las etapas de extracción, pre-procesado, procesado y modelado las que buscaremos optimizar en el contexto de todo el equipo, debido a que estas etapas se reparten entre sus miembros.</p>
<p>
<figure><img src="figures/lean_1.gif" /><figcaption>caption</figcaption>
</figure>
</p>
<p>Justificamos este cambio como parte de una búsqueda de procesos esbeltos (lean), que consisten en la eliminación de los "desperdicios", o fuentes de despilfarro de tiempo y trabajo en la elaboración de productos o servicios. A través de la solución que vamos a implantar buscamos la optimización continua del proceso y la aceptación de dicha cultura de optimización por el equipo.</p>
<p>Los puntos más importantes de los procesos "lean" son los siguientes:</p>
<ul>
<li>identificar los desperdicios y tratar de eliminarlos</li>
<li>mejorar la comunicación interna de la organización</li>
<li>reducir costes y tiempos de entrega y mejorar la calidad</li>
</ul></div><div id=justificación><h3>3 justificación</h3><ul>
<li>partimos de un rediseño de BD anterior.</li>
<li>buscamos automatizar los modelos de producción ()</li>
<li>necesitamos una arquitectura limpia.</li>
</ul>
<p>Necesidad de una reestructuración para la aplicación Agrai por su crecimiento para la gestión de grandes volúmenes de datos agrarios con especial énfasis en los índices vegetativos provenientes del procesamiento de imágenes satelitales. El estado actual de dicha aplicación consiste en la representación del estado de cierto parcelario a través del análisis de históricos de datos.</p></div><div id=alcance><h3>3 alcance</h3><p>El proyecto planteado inicialmente por la empresa tiene una planificación de cuatro meses. </p>
<p>A continuación, se expondrán los objetivos generales del proyecto con la intención de aportar una visión global del trabajo que se pretende realizar. Para evaluar el éxito de la realización de mi TFG se planteará en la fase de análisis de cada iteración unos requisitos a partir de los cuáles se podrá valorar el desarrollo de mi TFG y la influencia que tendrá mi aportación en el proyecto final.</p>
<p>Los objetivos finales del proyecto son:</p>
<ul>
<li>Automatizar el despliegue de la aplicación mediante infraestructura como código.</li>
<li>Diseñar un modelo de datos que permita un rápido escalado de dicha aplicación.</li>
<li>Automatizar el proceso de trasformación de los datos mediante el diseño de un pipeline.</li>
<li>Obtener un modelo de producción de cultivo para el parcelario (kg)</li>
<li>Incluir los resultados en la presentación de la aplicación.</li>
<li>Realizar un proceso de integración continua mediante pruebas de test para el pipeline desarrollado</li>
</ul></div><div id=metodología><h3>3 metodología</h3><p>En el equipo hacemos uso de SCRUM para la gestión del proyecto. Dicha metodología permite un desarrollo en cascada de las diferentes tareas propuestas para la creación del pipeline.</p>
<p>Se usará un ciclo de vida iterativo e incremental. Dentro de las fases del proyecto (también llamadas iteraciones), se repiten de manera intencionada una o más actividades del<br />
proyecto. Con estas iteraciones el entendimiento del producto por parte del equipo va  aumentando. Las fases (iteraciones) desarrollan el producto a través de una serie de ciclos repetidos, mientras que los incrementos van añadiendo sucesivamente funcionalidad al<br />
producto. En definitiva, consiste en varios ciclos de vida en cascada. Al final de cada iteración se entrega una versión mejorada.</p>
<p>
<figure><img src="figures/iteraciones.png" title="title" /><figcaption>caption</figcaption>
</figure>
</p></div><div id=EDT><h3>3 EDT</h3><div class="mermaid">
graph LR;
    TFG-->PLANIFICACIÓN;
    TFG-->ANÁLISIS;
    TFG-->DISEÑO;
    TFG-->IMPLEMENTACIÓN;
    PLANIFICACIÓN-->ALCANCE;
    PLANIFICACIÓN-->METODOLOGIA;
    PLANIFICACIÓN-->EDT;
    PLANIFICACIÓN-->ENTREGABLES;
    PLANIFICACIÓN-->TIEMPOS;
    PLANIFICACIÓN-->GANTT;
    PLANIFICACIÓN-->RRHH;
    PLANIFICACIÓN-->COMUNICACIONES;
    PLANIFICACIÓN-->RIESGOS;
    ANÁLISIS-->PREVIOS;
    ANÁLISIS-->REQUISITOS;
    REQUISITOS-->FUNCIONALES:
    REQUISITOS-->NO_FUNCIONALES;
    DISEÑO-->ARQUITECTURA;
    IMPLEMENTACIÓN-->PIPELINE;
    PIPELINE-->PIPE.1_D;
    PIPELINE-->PIPE.2_D;
</div></div><div id=entregables><h3>3 entregables</h3><p>Para acotar el proceso de trabajo identificamos los siguientes entregables que generaremos a lo a medida que avance el proyecto. Estos entregables quedarán en la memoria en sus correspondientes apartados o como anexos si tienen una extensión más larga. La siguiente tabla contiene que lleva a cada uno de estos artefactos.</p>
<table>
<thead>
<tr>
<th align="left">IDENT</th>
<th align="right">ENTREGABLE</th>
<th align="center">DESCRIPCIÓN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="https://github.com/alesteba/tfg/tree/main/entregables/pipeline">E01</a></td>
<td align="right">Módulo de Inicio</td>
<td align="center">Análisis de viabilidad que permitirá determinar si es posible desarrollar el proyecto.</td>
</tr>
<tr>
<td align="left">[E02]</td>
<td align="right">Planificación del proyecto</td>
<td align="center">Determinaremos los requisitos del proyecto, crearemos un enunciado para el alcance, realizaremos una descripción detallada de tareas junto a una EDT, estimaremos el tiempo y costo de los paquetes de trabajo, desarrollaremos un cronograma, estableceremos los estándares, procesos y métricas de calidad, determinaremos un plan de identificación de riesgos y crearemos el plan de gestión de cambios.</td>
</tr>
<tr>
<td align="left"><a href="https://github.com/alesteba/tfg/tree/main/entregables/pipeline">E03</a></td>
<td align="right">Infraestructura como código</td>
<td align="center">Desarrollo de un 'script' con las instrucciones bash para dejar la máquina que despliega la aplicación en un estado estable.</td>
</tr>
<tr>
<td align="left">[E04]</td>
<td align="right">Documento de Diseño</td>
<td align="center">Documento en el que se explican las decisiones tomadas para la creación de las entidades del modelo y sus relaciones.</td>
</tr>
<tr>
<td align="left"><a href="https://github.com/alesteba/tfg/tree/main/entregables/models.py">E05</a></td>
<td align="right">Esquema modelo de Datos</td>
<td align="center">Esquema UML que representa las entidades del modelo y sus relaciones. Modelo simplificado de datos exraido del ORM Django</td>
</tr>
<tr>
<td align="left">[E06]</td>
<td align="right">Arquitectura Pipeline</td>
<td align="center">Arquitectura y diagramas del proceso de transformación de datos, junto con sus scripts.</td>
</tr>
<tr>
<td align="left">[E07]</td>
<td align="right">Vista minable</td>
<td align="center">Estructura tabulada con las 'features' necesarias para el modelo de producción de cultivo.</td>
</tr>
<tr>
<td align="left">[E08]</td>
<td align="right">Modelo de producción</td>
<td align="center">Cuaderno jupyter en el que buscamos el mejor modelo posible</td>
</tr>
<tr>
<td align="left">[E09]</td>
<td align="right">Interfaz de usuario</td>
<td align="center">Entregable donde crearemos un apartado en la aplicación web AGRAI para mostrar las predicciones del modelo creado.</td>
</tr>
</tbody>
</table></div><div id=tiempos><h3>3 tiempos</h3><p>Para poder realizar un posterior seguimiento y control del proyecto asignamos a cada tarea un tiempo adecuado a la complejidad que estimamos para ésta. En caso de que posteriormente nos alejemos de lo que aquí planificamos, anotaremos dichas desviaciones en la sección de 'seguimiento y control'</p>
<table>
<thead>
<tr>
<th>V</th>
<th>Tarea</th>
<th>Horas</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0</td>
<td>DOP</td>
<td>10</td>
</tr>
<tr>
<td>1.1</td>
<td>INTRODUCCIÓN</td>
<td>1</td>
</tr>
<tr>
<td>1.2</td>
<td>ALCANCE</td>
<td>1</td>
</tr>
<tr>
<td>1.3</td>
<td>RECURSOS HUMANOS</td>
<td>1</td>
</tr>
<tr>
<td>1.4</td>
<td>COMUNICACIONES</td>
<td>1</td>
</tr>
<tr>
<td>1.5</td>
<td>METODOLOGIA</td>
<td>1</td>
</tr>
<tr>
<td>1.6</td>
<td>EDT</td>
<td>1</td>
</tr>
<tr>
<td>1.7</td>
<td>ENTREGABLES</td>
<td>1</td>
</tr>
<tr>
<td>1.8</td>
<td>ESTIMACIÓN DE TIEMPOS</td>
<td>1</td>
</tr>
<tr>
<td>1.9</td>
<td>GANTT</td>
<td>1</td>
</tr>
<tr>
<td>1.10</td>
<td>PLAN DE RIESGOS</td>
<td>1</td>
</tr>
<tr>
<td>2.0</td>
<td>ANÁLISIS</td>
<td>10</td>
</tr>
<tr>
<td>2.1</td>
<td>CONCEPTOS PREVIOS</td>
<td>1</td>
</tr>
<tr>
<td>2.2</td>
<td>PROYECTO AGRAI</td>
<td>1</td>
</tr>
<tr>
<td>2.3</td>
<td>PIPELINE</td>
<td>1</td>
</tr>
<tr>
<td>2.4</td>
<td>CATÁLOGO DE REQUISITOS</td>
<td>1</td>
</tr>
<tr>
<td>3.0</td>
<td>DISEÑ0</td>
<td>10</td>
</tr>
<tr>
<td>3.1</td>
<td>ANÁLISIS MODELO PREVIO</td>
<td>1</td>
</tr>
<tr>
<td>3.2</td>
<td>REDISEÑO ENTIDADES</td>
<td>1</td>
</tr>
<tr>
<td>3.3</td>
<td>CARGA CON DATOS</td>
<td>1</td>
</tr>
<tr>
<td>3.4</td>
<td>DESPLIEGUE DE APLICACIÓN</td>
<td>1</td>
</tr>
<tr>
<td>3.4</td>
<td>PIPELINE</td>
<td>1</td>
</tr>
<tr>
<td>4.0</td>
<td>IMPLEMENTACIÓN</td>
<td>10</td>
</tr>
<tr>
<td>4.1</td>
<td>DATOS DE ENTRADA</td>
<td>1</td>
</tr>
<tr>
<td>4.2</td>
<td>DEFINICIÓN DE ETAPAS</td>
<td>1</td>
</tr>
<tr>
<td>4.3</td>
<td>VISTA MINABLE</td>
<td>1</td>
</tr>
<tr>
<td>4.4</td>
<td>MODELO DE PRODUCCIÓN</td>
<td>1</td>
</tr>
<tr>
<td>4.4</td>
<td>AUTOMATIZACIÓN COMPLETA</td>
<td>1</td>
</tr>
<tr>
<td>5.0</td>
<td>PRUEBAS</td>
<td>10</td>
</tr>
<tr>
<td>5.1</td>
<td>PRUEBAS</td>
<td>1</td>
</tr>
<tr>
<td>6.0</td>
<td>MEMORIA</td>
<td>10</td>
</tr>
<tr>
<td>6.1</td>
<td>ESTUDIO PREVIO</td>
<td>1</td>
</tr>
<tr>
<td>6.2</td>
<td>ESTRUCTURA</td>
<td>1</td>
</tr>
<tr>
<td>6.3</td>
<td>REDACCIÓN</td>
<td>1</td>
</tr>
<tr>
<td>6.4</td>
<td>REVISIÓN</td>
<td>1</td>
</tr>
<tr>
<td>7.0</td>
<td>SEGUIMIENTO Y CONTROL</td>
<td>10</td>
</tr>
<tr>
<td>7.1</td>
<td>GESTIÓN DE TIEMPOS</td>
<td>1</td>
</tr>
<tr>
<td>7.2</td>
<td>REUNIONES</td>
<td>1</td>
</tr>
</tbody>
</table></div><div id=gantt><h3>3 gantt</h3><p>https://gist.github.com/martinwoodward/8ad6296118c975510766d80310db71fd</p>
<p>El siguiente diagrama propone un desglose de tareas para la planificación del proyecto. </p>
<div class="mermaid">
gantt
    title Planificación
    dateFormat  DD-MM
    axisFormat  %d

    section ANALISIS
    ALCANCE: done, 01-01, 2d
    METODOLOGIA: crit, 02-01, 3d
    EDT: 03-01, 5d
    GANNT: 04-01, 5d
    RRHH: 05-01, 3d
    COMUNICACIONES: 06-01, 5d
    RIESGOS: 07-01, 5d

    section MODELO_DATOS
    RELACIONES BD: done, 08-01, 15d
    REDISEÑO: crit, 09-01, 15d

    section ENTORNO
    DJANGO+JUPYTER: done, 10-01, 2d
    INT_CONTINUA: crit, 11-01, 3d

    section PIPELINE
    INFR: 20-01, 5d
    LOAD: 21-01, 5d
    FEAT: 25-01, 5d
    VIEW: 25-01, 7d
    MODEL: 30-01, 7d
    WEB: 30-01, 5d
    Phase 2 complete: milestone, 30-01, 0d

    section TEST
    TEST 1: 20-01, 30d
    TEST 2: 30-01, 20d
    TEST 3: milestone, 20-02, 1d

    section MEMORIA
    MEM_WRITE: 01-01, 60d
    SEG_Y_CONT: 01-01, 60d


    Project complete: milestone, 10-03, 0d
</div>

<p>Este diagrama es una estipulación de cómo creo que voy a ir durante el desarrollo del modelo de datos y del pipeline correspondiente. En la sección final de seguimiento y control analizamos las desviaciones encontradas y cómo han sido solucionados estos contratiempos.</p></div><div id=rrhh><h3>3 rrhh</h3><p>El personal implicado en el Trabajo de Fin de Grado será:  </p>
<table>
<thead>
<tr>
<th align="left">INTERESADO</th>
<th align="center">LABOR</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Alberto Esteban Larreina</td>
<td align="center">Ejecutor y responsable del proyecto. Desarrollará el proyecto en su totalidad</td>
</tr>
<tr>
<td align="left">Antonio Rúbio</td>
<td align="center">Tutor en la empresa. Se dedicará a corregir los aspectos técnicos del proyecto referentes al desarrollo  del mismo y planificación.</td>
</tr>
<tr>
<td align="left">Jónathan Heras</td>
<td align="center">Tutor académico. Se dedicará a corregir aspectos referentes a la documentación y la forma en la que se desarrollará el proyecto a lo largo del tiempo estipulado y me guiará a cerca del desarrollo de este.</td>
</tr>
</tbody>
</table></div><div id=comunicaciones><h3>3 comunicaciones</h3><p>Para mantener informados tanto al tutor académico como a la tutora de la empresa, utilizaremos los siguientes canales:  </p>
<table>
<thead>
<tr>
<th align="left">CANAL</th>
<th align="right">DESCRIPCIÓN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Reuniones</td>
<td align="right">presenciales, tanto con el tutor académico como con el tutor de la empresa.</td>
</tr>
<tr>
<td align="left">Email</td>
<td align="right">se utilizará como forma de comunicación electrónica para concretar determinadas pautas o establecimiento de citas.</td>
</tr>
<tr>
<td align="left">Discord</td>
<td align="right">aplicación de comunicación interna para mensajes directos con los miembros del equipo.</td>
</tr>
</tbody>
</table></div><div id=riegos><h3>3 riegos</h3><p>El objetivo de esta tabla es aumentar la probabilidad de eventos positivos y disminuir la de los negativos.</p>
<table>
<thead>
<tr>
<th align="left">FUENTE</th>
<th align="center">RIESGO</th>
<th align="center">SI SUCEDE</th>
<th align="center">MINIMIZAR</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Ausencia de interesados</td>
<td align="center">Médio</td>
<td align="center">this is centered</td>
<td align="center">this is centered</td>
</tr>
<tr>
<td align="left">Ausencia del Tutor</td>
<td align="center">these will be justified</td>
<td align="center">this is centered</td>
<td align="center"></td>
</tr>
<tr>
<td align="left">Tecnología equivocada</td>
<td align="center">Medio</td>
<td align="center">Estudiar nuevas posiblidades</td>
<td align="center">Uso de patrones abstractos elegantes que se puedan implementar en diferentes frameworks</td>
</tr>
<tr>
<td align="left">Pérdida código fuente</td>
<td align="center">Alto</td>
<td align="center">Empezar casi de cero</td>
<td align="center">Backups y control de versiones</td>
</tr>
<tr>
<td align="left">Pérdida documentación</td>
<td align="center">Alto</td>
<td align="center">Empezar casi de cero</td>
<td align="center">Utilizar herramientas de copias incrementales</td>
</tr>
<tr>
<td align="left">Fallo del servidor con la applicación</td>
<td align="center">Alto</td>
<td align="center">Utilizar y configurar uno nuevo</td>
<td align="center">Backups</td>
</tr>
<tr>
<td align="left">Falta de horas para terminar el proyecto</td>
<td align="center">Medio</td>
<td align="center">Dedicarse primero a los requisitos más importantes.</td>
<td align="center">Realizar una buena planificación y ajustar el alcance</td>
</tr>
</tbody>
</table>
<p>De la tabla anterior se prestará especial atención a los riesgos valorados como 'Alto', ya que por su incidencia en el proyecto requieren un seguimiento exhaustivo, para los cuales se desarrollarán sus correspondientes planes de contingencia.  </p>
<p>Los riesgos de valoración Media y Baja tendrán un control basado en su plan de mitigación simple de seguimientos al cronograma, con intervalos cortos de tiempo.</p></div><div id=AGRAI-data><h3>3 AGRAI-data</h3><p>AGRAI es una aplicación para la gestión de cultivo que permite al agricultor monitorizar el estado de sus parcelas. El estado actual de la aplicación se centra el despliegue de datos geo-referenciados por medio de una interfaz web. Los resultados para el usuario de la aplicación son satisfactorios permitiendo a éste consultar el estado de su parcelario junto con algunas predicciones en cualquier momento. </p>
<p>En la interfaz web el cliente observa un mapa con sus parcelas resaltadas en colores. Este color reflejado en  los píxeles hace referencia a los índices vegetativos provenientes de las imágenes satelitales. Los clientes utilizan dicha interfaz para ver el estado de sus cultivos y observar la producción que predicen los modelos de Inteligencia Artificial generados.</p>
<p>
<figure><img src="figures/visor_GIS.png" /><figcaption>caption</figcaption>
</figure>
</p>
<p>A nivel interno, el equipo trabaja con datos provenientes de imágenes descargadas de satélites como SENTINEL, o de grabaciones realizadas por dron cuando se requiere una mayor calidad. Estos datos se mezclan con información proporcionada por estaciones meteorológicas como el SIAR y con la información que los clientes pueden proporcionar sobre campañas anteriores.</p>
<p>Por otro lado, cómo actualmente se procesa la información es un proceso tedioso para el equipo. Los diferentes miembros trabajan con tecnologías distintas sobre datos muchas veces duplicados que provienen de fuentes comunes.</p>
<p>El objetivo de este trabajo de fin de grado es mejorar y automatizar el flujo de trabajo del equipo convirtiendo la aplicación AGRAI en una herramienta robusta que manipule un único repositorio de datos al cuál el resto del equipo pueda acceder y utilizar de forma segura y lo más importante, sin duplicar y romper la integridad de los datos.</p>
<p>Entiendo que es un proyecto ambicioso debido a que no sólo es importante el conocimiento técnico sino que serán necesarios cambios en la forma de trabajo del equipo además de la confianza de cada miembro por la nueva forma de trabajo que se desea implementar. Intentaremos seguir los principios de los procesos 'lean' para buscar la mejor optimización continua.</p>
<p>
<figure><img src="figures/lean.png" title="title" /><figcaption>caption</figcaption>
</figure>
</p></div><div id=ideas-previas><h3>3 ideas-previas</h3><p>Mencionábamos la búsqueda de optimización del proceso como una de las principales idea para mejorar el flujo de datos que utiliza la aplicación. El rediseño que se quiere implantar y la arquitectura resultante tiene que cumplir con los principios de dicha metodología. Para que este trabaja resulte satisfactorio, el software tiene que poder utilizarse por el equipo con agilidad, consiguiendo que quede como una herramienta física que coordine el esquema mental de los participantes que la usan.</p>
<p>Para poder realizar este proyecto decidimos trabajar con un número pequeño representativo de datos de la aplicación actualmente en producción. Un menor volumen de información permite realizar pruebas y da pie a fijarnos en las relaciones y los esquemas estructurales con más precisión.  </p>
<p>Como este trabajo se realiza en equipo, a medida que voy haciendo pruebas con mi entorno local, un compañero se encarga de ir probando mi entorno con los volúmenes originales de datos que la aplicación utiliza en producción. La reestructuración del modelo de BD que realizamos contempla un posterior escalado de la aplicación que permitirá la integración continua con más datos y nuevas tecnologías.</p>
<p>La siguiente tabla muestra las principales fuentes de datos de las que se obtienen y enlaza la información. El volumen de datos que se puede llegar a manejar es grande, por cada parcela se persisten varios índices vegetativos en cada uno de sus píxeles. </p>
<table>
<thead>
<tr>
<th align="left">FUENTE DATOS</th>
<th align="left">DESCRIPCIÓN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">QGIS</td>
<td align="left">información geomométrica de parcelas y sus pixéles</td>
</tr>
<tr>
<td align="left">Índices Vegetativos</td>
<td align="left">provenientes de imágenes satelitales descargadas en diferentes fechas</td>
</tr>
<tr>
<td align="left">Cultivos / Variedad</td>
<td align="left">información tabulada en excels sobre tipo de cultivos y sus variedades</td>
</tr>
<tr>
<td align="left">RF3</td>
<td align="left">documentar el proceso de desarrollo generando los documentos de diseño pertinente</td>
</tr>
</tbody>
</table>
<p>Para poder realizar el trabajo utilizaremos una muestra representativa de los datos debido a que el proceso de descarga de índices y persistencia de datos es largo para realizar las pruebas. El sistema está pensado para trabajar con muchas parcelas, en las pruebas que yo voy a realizar escogemos una muestra de 25 parcelas y pensaremos una descarga de índices para no más de 4 fechas diferentes.</p>
<p>Es el histórico de índices en diferentes fechas lo que permite al sistema realizar modelos predictivos. A mayor volumen de datos, más precisión podremos obtener en los modelos posteriormente. El punto importante de este trabajo es la mejora del proceso y la automatización del flujo de datos, por ello no será relevante que los modelos predictivos que creemos al final no tengan buena precisión. </p>
<p>Para hacernos una idea, contemplando solo 25 parcelas, podemos almacenar 3000 píxeles. Por cada pixel vamos a registrar varios índices vegetativos (ndvi, ndre) y para generar el histórico de datos esta información se multiplica por el número de fechas contempladas. Es decir, que aunque trabajamos sobre un volumen reducido para probar la automatización, siguen siendo mucha información que tiene que persistirse en la base de datos.</p></div><div id=automatizacion><h3>3 automatizacion</h3><p>El proceso que vamos a implantar requiere de un diseño que posibilite estructurar y manejar grandes volúmenes de datos. Actualmente existen varias tecnologías que abordan el concepto de Pipeline desde un punto de vista global. No vamos a utilizar ninguna de estas tecnologías, ETL, nuestro proceso se se va ha desarrollar a medida, utilizando un Pipeline como una parte de la arquitectura, encargada de la transformación de los datos hasta su almacenamiento en el modelo.</p>
<p>Este proceso de diseño va a ser casi manual.</p></div><div id=tecnologías><h3>3 tecnologías</h3><p>El código de la aplicación que heredamos está escrito en Python. El lenguaje es una decisión adecuada debido a la necesidad de integrar técnicas de procesamiento de datos e inteligencia artificial. El framework de Django para Python permite construir un proyecto robusto y despegarlo en un servidor con una interfaz web.</p>
<p>Django nos ofrece las herramientas necesarias para trabajar desde un alto nivel de abstracción y poder diseñar una aplicación sólida y estable. Algunas de estas herramientas son:</p>
<ul>
<li>ORM, Object-Relational-Mapping: permite crear un modelo de datos y gestiona automáticamente la BD (base de datos), subyacente. Abstrae las consultas SQL y evita tener que realizar migraciones manuales de los esquemas.</li>
<li>Static File Generator: podemos diseñar la interfaz de la aplicación en formato web y desplegar en un servidor.</li>
<li>Commands-System: gestión de comandos internos mediante los que se pueden automatizar tareas, utilizaremos esta arquitectura para diseñar el pipeline de datos y persistir la información proveniente de diferentes fuentes.</li>
</ul>
<p>Aunque el framework es muy potente, harán falta otras herramientas y entornos para completar con éxito la automatización que buscamos y así conseguir un proceso de optimización continua. En el siguiente punto hablaremos de la infraestructura que la aplicación requiere y de cómo podemos solventar algunos de los problemas de integración más importantes.</p>
<p>
<figure><img src="figures/django_migrate.png" title="original" /><figcaption>caption</figcaption>
</figure>
</p>
<p>La utilización de Python, aparte de incluir este framework, nos da la posibilidad de utilizar las librerías de inteligencia artificial y ciencia de datos que finalmente utilizaremos para la creación del modelo de cultivo. Algunas de librerías principales que vamos a utilizar son las siguientes:</p>
<table>
<thead>
<tr>
<th align="left">LIBRERÍA</th>
<th align="left">DESCRIPCIÓN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">scikit-learn</td>
<td align="left">creación de modelos de inteligencia artificial</td>
</tr>
<tr>
<td align="left">numpy</td>
<td align="left">tratamiento de datos multidimensionales</td>
</tr>
<tr>
<td align="left">pandas</td>
<td align="left">uso de datos tabulados con herramientas para su procesamiento</td>
</tr>
</tbody>
</table></div><div id=requisitos><h3>3 requisitos</h3><p>La siguiente tabla muestra los requisitos funcionales críticos para completar con éxito el proyecto, terminando el trabajo con un producto estable que el equipo pueda utilizar.</p>
<table>
<thead>
<tr>
<th align="left">REQUISITO</th>
<th align="left">DESCRIPCIÓN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">RF1</td>
<td align="left">automatizar el despliegue de la aplicación identificando las librerías y dependencias necesarias</td>
</tr>
<tr>
<td align="left">RF2</td>
<td align="left">rediseñar el modelo de datos para poder escalar la aplicación y su gestión de datos.</td>
</tr>
<tr>
<td align="left">RF3</td>
<td align="left">documentar el proceso de desarrollo generando los documentos de diseño pertinentes.</td>
</tr>
<tr>
<td align="left">RF4</td>
<td align="left">creación de un entorno común que permita trabajar al equipo sobre las mismos fuentes de datos.</td>
</tr>
<tr>
<td align="left">RF5</td>
<td align="left">obtención de una vista minable con la selección de features provenientes del modelo.</td>
</tr>
<tr>
<td align="left">RF6</td>
<td align="left">creación de un modelo de producción probando varios algoritmos de inteligencia artificial.</td>
</tr>
<tr>
<td align="left">RF7</td>
<td align="left">despliegue de los datos del modelo en la interfaz de la aplicación.</td>
</tr>
</tbody>
</table>
<p>Como requisitos no funcionales para la aplicación identificamos los siguientes, entendiendo que se completarán a lo largo de todo el proyecto.</p>
<table>
<thead>
<tr>
<th align="left">REQUISITO</th>
<th align="left">DESCRIPCIÓN</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">RNF1</td>
<td align="left">Crear un entorno Linux local con la misma configuración necesaria en producción.</td>
</tr>
<tr>
<td align="left">RNF2</td>
<td align="left">Preparar las fuentes de datos con una muestra pequeña representativa de la aplicación en producción.</td>
</tr>
<tr>
<td align="left">RNF3</td>
<td align="left">Estudiar las posibles features para la vista minable que utilizará el modelo de producción de cultivo</td>
</tr>
<tr>
<td align="left">RNF4</td>
<td align="left">Implantar un flujo de trabajo en el equipo para utilizar las mismas fuentes de datos</td>
</tr>
<tr>
<td align="left">RNF5</td>
<td align="left">Utilizar software libre</td>
</tr>
</tbody>
</table></div><div id=infraestructure><h3>3 infraestructure</h3><div class="mermaid">
graph LR;    
    subgraph desarollo
        WSL--> WSL.1
        WSL--> WSL.2
        WSL--> WSL.3
    end

    subgraph producción

        WSL.1--> Ubuntu
        WSL.2--> Ubuntu
        WSL.3--> Ubuntu

    end


        BD_test-->WSL
        BD_prod-->Ubuntu


    subgraph repositorio
        CODE --> GIT    
        GIT --> WSL
        GIT --> Ubuntu
    end
</div></div><div id=entorno><h3>3 entorno</h3><p>Es importante preparar un entorno común para el equipo. Django es un framework muy útil por la integración de las herramientas que hemos visto (ORM, web-interfaces, etc.), pero para conseguir un workflow adecuado necesitamos incluir otras herramientas propias de ciencia de datos como pueden ser cuadernos de jupyter y sus entornos con las librerías necesarias para ejecutar modelos predictivos.</p>
<p>Migración de todo el entorno a UNIX para posteriormente utilizar herramientas de integración continua. El trabajo en el equipo local se hace a través de una máquina en WSL (windows subsystem for Linux) para trabajar de forma semejante a un contender. Uno de los problemas principales para el equipo en su flujo de trabajo ha sido mantener una integridad entre el desarrollo realizad por los diferentes miembros del equipo y el posterior despliegue de la aplicación en un entorno UNIX diferente al que se había utilizado en local en las versiones de pre-producción.</p>
<p>Dedicamos varios días a preparar la integración de Django con cuadernos jupyter y además poder ejecutar diferentes entornos virtuales con las librerías adecuadas en cada momento. Los siguientes ejemplos muestran como se ha conseguido integrar las librerías necesarias para el despliegue de los cuadernos y su integración con los datos y los modelos de Django.</p>
<p>Los cuadernos jupyter que se encuentran en esta memoria se han exportado desde la propia aplicación. Vemos la facilidad con la que el equipo puede redactar informes para clientes concretos.  Estos cuadernos son un punto tan importante de la arquitectura como puede ser el módulo de servicios.</p></div><div id=UNIX><h3>3 UNIX</h3><p>Hacemos uso de WSL (Windows Subsystem for Linux) para crear el entorno de desarrollo necesario para el proyecto. De esta forma conseguimos que los entornos de pruebas y producción sean muy similares, permitiendo automatizar el despliegue de la aplicación mediante técnicas de infraestructura como código. Una de las ventajas de tener una máquina Ubuntu como entorno de desarrollo es que mediante un script .sh instalamos todas las dependencias necesarias para dejar dicha máquina en un estado estable preparado para el despliegue de la aplicación.</p>
<p>Para el trabajo interno de la aplicación necesitamos entornos con librerías más pesadas como numpy o scikit-learn que no deberíamos usar en la producción de solo la interfaz web, ya que ésta simplemente muestra los datos del parcelario registrado con su producción estimada. Consideramos la posibilidad de crear varios entornos virtuales para los diferentes flujos de trabajo que contempla la aplicación.</p>
<ul>
<li>Despliegue de la aplicación</li>
<li>data-science para la creación de modelos a nivel interno.</li>
</ul>
<p>Actualmente solo existe un entono virtual sobre el cuál se instalan todas las librerías python necesarias para el desarrollo y despliegue. Dicha refactorización sobre la infraestructura se contempla para próximos hitos en el proyecto. Es importante identificar aquellos puntos que sean propensos de un refactor grande, pero que no se lleven a cabo en el momento actual. Una vez identificados valoramos su importancia y vemos si el diseño actual impediría su mejora. En este caso sería muy sencillo, una vez crezca la aplicación separar los entornos virtuales para mejorar el tiempo de build y deployment.</p></div><div id=control-versiones><h3>3 control-versiones</h3><div class="mermaid">
       gitGraph
           commit
           branch origins
           commit tag:"v1.0.0"
           commit
           checkout main
           commit type: HIGHLIGHT
           commit
           merge origins
           commit
           branch featureA
           commit
</div></div><div id=componentes><h3>3 componentes</h3><div class="mermaid">
erDiagram

    CUADERNO ||--|{ CORE : uses
    ROLES }|..|{ CORE : uses
    METEO }|..|{ CORE : uses
    AUTO }|..|{ CORE : uses
</div></div><div id=data-load><h3>3 data-load</h3><div class="mermaid">
erDiagram
    AUTO }|..|{ descarga_indices : contains
    AUTO }|..|{ CORE : uses
</div></div><div id=modelo-BD><h3>3 modelo-BD</h3><div class="mermaid">
erDiagram
    INDICE ||--o{ MIRAR_INDICE : contains
    INDICE {
        string nombre FK
        string descripcion
    }
    PIXEL ||--o{ MIRAR_INDICE : contains
    PIXEL ||--o{ PARCELA : contains
    PIXEL {
        string parcela FK 
        polygon geom
        string idx
    }
    PARCELA {
        string idx 
        string estacion
        float altitud
        polygon geom
    }
    MIRAR_INDICE {
        string indice FK
        string pixel FK
        date fecha
        geojson json
        float valor
    }
</div></div><div id=services><h3>3 services</h3><div class="mermaid">
       gitGraph
           commit
           branch origins
           commit tag:"v1.0.0"
           commit
           checkout main
           commit type: HIGHLIGHT
           commit
           merge origins
           commit
           branch featureA
           commit
</div></div><div id=despliegue><h3>3 despliegue</h3><p>Con la aplicación en un estado sólido, procedemos a mostrar los datos en la interfaz web. Aunque no es parte del trabajo establecido para este TFG, necesitamos ver que los datos con los que trabajamos se muestran correctamente en el visor GIS de nuestra interfaz. </p>
<p>Recordemos que dentro de nuestras fuentes de datos tenemos datos de parcelas con su información geoespacial, tanto la forma de la parcela como la de todos sus píxeles que la conforman. Como estamos haciendo pruebas con un número reducido de datos, 25 parcelas con sus píxeles e índices asociados, el visor debe mostrar estas 25 parcelas con diferentes colores dependiendo del valor de los indices estudiados.</p>
<ul>
<li>captura visor *</li>
</ul></div><div id=previos-pipe><h3>3 previos-pipe</h3><p>Es importante diferenciar qué tipo de estructura estamos construyendo. Existen actualmente varias arquitecturas para 'pipelines' como pueden ser ETLs u otras. Para la correcta carga y transformación de datos no utilizaremos ninguna de estas arquitecturas, sino que diseñaremos los bloques necesarios en cada paso para poblar nuestra base de datos. Este proceso puede entenderse como una canalización de datos, en la que recibimos información en diferentes fuentes y la dotamos de contexto dentro de la BD.</p>
<h2>¿Qué es una canalización de datos?</h2>
<p>Una canalización de datos se refiere a los pasos necesarios para mover datos del sistema de origen al sistema de destino. Estos pasos incluyen copiar datos, transferirlos desde una ubicación en el sitio a la nube y combinarlos con otras fuentes de datos. El objetivo principal de una canalización de datos es garantizar que todos estos pasos se produzcan de forma coherente con todos los datos.</p>
<p>En el apartado anterior hemos visto cómo se ha diseñado el modelo de datos. Ahora nos centramos en los pequeños pasos de carga que vamos a dar para que los datos agronómicos de clientes e imágenes satelitales se persistan en dicho modelo. Valoramos la identificación de unidades atómicas de información que puedan persistirse reiteradamente. Es decir, buscaremos acotar pequeños procesos de carga que puedan ejecutarse en varios puntos dependiendo del volumen de datos de clientes que maneje el equipo en un momento dado.</p>
<h2>Procesos de una canalización</h2>
<p>Identificamos tres conceptos que definen la canalización de datos que vamos a llevar a cabo y exponemos que hace el 'pipeline' de nuestra aplicación en cada uno de ellos.  </p>
<ul>
<li>
<p>Data Ingestion: diseñamos bloques atómicos de carga que persisten datos de fuentes diferentes en la base de datos de la aplicación.</p>
</li>
<li>
<p>Data Transformation: utilizamos los bloques de carga para persistir los datos en nuestro modelo de Django.</p>
</li>
<li>
<p>Data Storage: almacenamos los datos en el modelo a través de Django y manteniendo una base de datos relacional en la máquina en la que se encuentra la aplicación.</p>
</li>
</ul>
<p>Como hemos dicho, hay varias formas de realizar este proceso de canalización, en el siguiente punto exponemos la tecnología subyacente que utilizaremos para ello. </p></div><div id=commands-CLI><h3>3 commands-CLI</h3><div class="mermaid">
graph LR;
    indices --> download_indices;
    download_indices --> download_today
    download_indices --> download_range
</div></div><div id=model-view><h3>3 model-view</h3><p>Hasta este punto habíamos insertado en la base da datos la información que provenía de diferentes fuentes. Hemos dotado de consistencia y relación a los datos y se ha creado un sistema con capacidad para hacer consultas sobre estos de forma ordenada y sencilla.</p>
<p>Ahora, fuera de la estructura de comandos de Django y utilizando el entorno virtual creado y los cuadernos integrados en la aplicación, escribimos el algoritmo para extraer los datos y dar la forma necesaria para los siguientes pasos.</p>
<p>El código que mostramos en este apartado corresponde con la generación de la vista tabulada con la que vamos a crear el modelo de producción de cultivo. Ahora realizamos el proceso inverso a la persistencia de datos que hemos realizado hasta este punto. Utilizamos la información y sus relaciones persistidas en la base de datos de nuestro sistema para generar la vista minable que necesita el modelo predictivo. </p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">statistic_indices</span><span class="p">(</span><span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ndvi&#39;</span><span class="p">,</span> <span class="s1">&#39;ndre&#39;</span><span class="p">],</span> <span class="n">func</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">):</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="n">col_ids</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">Parcela</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>

        <span class="n">col_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>

    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;IDX&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">col_ids</span>

    <span class="c1"># para cada indice:</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>

        <span class="n">indice_p</span> <span class="o">=</span> <span class="n">Indice</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">nombre</span><span class="o">=</span><span class="n">ind</span><span class="p">)</span> <span class="c1"># if type(indice_p) == str else indice_p</span>

        <span class="k">for</span> <span class="n">fecha</span> <span class="ow">in</span> <span class="n">fechas</span><span class="p">:</span> 

            <span class="n">col_data</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># por cada iteración de esto tienes una fila en el df:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">Parcela</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>

                <span class="c1"># la media de todos sus índices</span>
                <span class="n">p_indices</span> <span class="o">=</span> <span class="n">Pixel</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">parcela</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>

                <span class="n">list_values</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="k">for</span> <span class="n">p_itr</span> <span class="ow">in</span> <span class="n">p_indices</span><span class="p">:</span>

                    <span class="n">qs</span> <span class="o">=</span>  <span class="n">Mirar_Indice</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>

                        <span class="n">pixel</span> <span class="o">=</span> <span class="n">p_itr</span><span class="p">,</span> 
                        <span class="n">indice</span> <span class="o">=</span> <span class="n">indice_p</span><span class="p">,</span> 
                        <span class="n">fecha</span> <span class="o">=</span> <span class="n">fecha</span> <span class="c1"># fecha para la columna:</span>
                    <span class="p">)</span>

                    <span class="kn">import</span> <span class="nn">math</span>

                    <span class="k">if</span> <span class="p">(</span> <span class="ow">not</span> <span class="n">math</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">qs</span><span class="o">.</span><span class="n">valor</span><span class="p">)</span> <span class="p">):</span>

                        <span class="n">list_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">qs</span><span class="o">.</span><span class="n">valor</span><span class="p">)</span>

                <span class="c1"># estadistico:añadir columna</span>

                <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">list_values</span><span class="p">)</span>

                <span class="n">col_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

            <span class="n">df</span><span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;_&#39;</span> <span class="o">+</span> <span class="n">ind</span> <span class="o">+</span><span class="s1">&#39;_&#39;</span> <span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">fecha</span><span class="p">)]</span> <span class="o">=</span> <span class="n">col_data</span>

            <span class="c1"># valor a la tabla</span>

    <span class="k">return</span> <span class="n">df</span>
</code></pre></div>

<h2>Selección de features</h2>
<p>El diseño de la vista minable es complicado, para ello trabajo con el equipo seleccionando los campos de la BD más importantes que formaran la tabla.  Nos centramos principalmente en los índices vegetativos registrados en diferentes fechas. </p>
<p>Para poder predecir la producción de cultivo incluimos en la vista varios estadísticos como la media y el sumatorio para cada índice en cuatro fechas representativas de la evolución de este. El diseño de esta estructura proviene de un estudio previo realizado por el equipo, en el que se ha concluido que ciertos estadísticos en unas fechas calibradas funcionan bien para predecir la producción. </p>
<p>Hablábamos anteriormente de entornos virtuales, por el momento la aplicación utiliza un solo entorno virtual Python, pero este punto del trabajo podría hacer uso de un entorno separado con las librerías de ciencia de datos instaladas, de tal forma separaríamos dos procesos importantes. Resaltar que en el trabajo me estoy encargando del proceso para hacer pruebas, pero serán diferentes miembros del equipo los que usen estas tecnologías y para que podamos seguir escalando a medida que evolucione la aplicación, estos miembros tienen que poder utilizar los entornos creados.</p>
<h2>Variable Objetivo</h2>
<p>Junto con los datos extraídos añadimos la variable objetivo de producción. Esta variable, todavía sin persistir en el modelo, hace referencia a los kilos de producción de cada parcela en su cosecha.</p></div><div id=modelo-producción><h3>3 modelo-producción</h3><p>La vista minable que hemos obtenido representa los datos necesarios para crear un modelo de producción para el cultivo. El <a href="https://github.com/alesteba/tfg/tree/main/entregables">próximo entregable</a> es el desarrollo de uno o varios cuadernos de jupyter con modelos de inteligencia artificial para la tabla anterior.</p>
<p>En el punto en el que estamos podemos pensar en las múltiples vistas que se pueden generar a partir de los datos persistidos en el BD. Desde aquí desarrollaremos la automatización de la búsqueda del mejor modelo posible para la vista anterior, pero siempre teniendo en cuenta que a partir de los datos persistidos podemos predecir muchos otros valores, no solo la producción de un cultivo.</p>
<p>Como los datos con los que trabajamos en los cuadernos provienen de una base de datos estructurada y previamente estudiada, no va a hacer falta un paso de preprocesamiento previo. 
Sí que intentaremos que los datos de producción con los que vamos a predecir estén normalizados y se haya hecho una búsqueda previa de outliers.</p>
<p>Buscamos un modelo de regresión para el número de Kg de cultivo en cada parcela. Aunque los datos son representativos de un parcelario pequeño, diseñaremos los cuadernos para automatizar la búsqueda del mejor modelo con cualquier volumen de datos que podamos necesitar más adelante.</p>
<p>El rango de valores para la mayoría de los índices que obtenemos va de -1 a 1. Los datos de índices negativos que llegan a la vista se pueden considerar como outliers. Esto es debido a que si los índices provienen de una imagen con nubes, no se diferencian los colores del terreno y el índice acaba teniendo un valor muy malo. Para evitar valores corruptos eliminaremos los índices que provengan de la toma de la imagen en una fecha en la que había nubes.</p>
<p>
<figure><img src="figures/nubes.png" /><figcaption>caption</figcaption>
</figure>
</p>
<p>El <a href="https://github.com/alesteba/tfg/tree/main/entregables">siguiente repositorio</a> contiene los cuadernos necesarios para la gestión y automatización de los modelos de producción generados a partir los datos de las 25 parcelas estudiadas.</p></div><div id=seguimiento><h3>3 seguimiento</h3><p>A pesar de realizar la planificación del proyecto, el desarrollo ha diferido de la misma en<br />
algunas ocasiones. Esta sección está dedicada a uno de los puntos más importantes en el<br />
desarrollo de proyectos: el seguimiento y control.  </p>
<p>El número de horas planificadas era de 300, las cuales se repartían en 7 etapas marcadas<br />
por los diversos puntos de control: Planificación, análisis, diseño, implementación,<br />
pruebas, memoria y seguimiento del trabajo.  </p>
<p>A continuación, podremos ver en la tabla 4 las horas estimadas respecto a las reales junto<br />
con las horas de desviación de cada uno de los paquetes de trabajo.</p></div><div id=memoria><h3>3 memoria</h3><p>cómo se ha escrito y por qué.</p></div><div id=lecciones><h3>3 lecciones</h3><p>El objetivo de este trabajo no es obtener un producto software final sino la optimización de un proceso o flujo de trabajo. Los cambios planteados mejoran el acceso a los datos y la forma de procesar estos por el resto de participantes del equipo. </p>
<p>Durante el avance del proyecto se han ido documentando algunas lecciones aprendidas en el siguiente blog. Estas pequeñas notas documentan algunos de los problemas que pueden volver a aparecer. La mayoría de estas publicaciones son relacionadas con problemas de arquitectura de software relacionados con el framework que se está utilizando, Django. Además encontramos código que permite automatizar la visualización de ciertos procesos que ayudan a optimzar el proceso de trabajo.</p>
<ul>
<li>blog-post 1</li>
<li>blog-post 2</li>
<li></li>
</ul></div><div id=conclusiones><h3>3 conclusiones</h3><p>Este trabajo se ha centrado en el desarrollo de un proceso de automatización que permite la integración continua del flujo de trabajo del equipo. Hemos pasado por casi todas las etapas de diseño e implementación además de la final inclusión de los modelos predictivos para el cultivo.</p>
<p>Revisamos el proceso ....</p></div><div id=bibliografía><h3>3 bibliografía</h3><p>http://gorodinski.com/blog/2012/04/14/services-in-domain-driven-design-ddd/</p>
<p>https://medium.com/@bonnotguillaume/software-architecture-the-pipeline-design-pattern-from-zero-to-hero-b5c43d8a4e60</p>
<p>https://mermaid.js.org/syntax/entityRelationshipDiagram.html</p>
<p>https://docs.djangoproject.com/en/4.1/intro/tutorial01/</p>
<p>https://simpleisbetterthancomplex.com/tutorial/2018/08/27/how-to-create-custom-django-management-commands.html</p>
<p>https://www.astera.com/es/type/blog/etl-pipeline-vs-data-pipeline/</p>
<p>https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/</p>
<p>https://towardsdatascience.com/random-forest-regression-5f605132d19d</p>
<p>https://www.kaggle.com/code/sociopath00/random-forest-using-gridsearchcv</p>
<p>https://plotly.com/python/knn-classification/</p>
<p>https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_pipeline_display.html#sphx-glr-auto-examples-miscellaneous-plot-pipeline-display-py</p>
<p>https://scikit-learn.org/stable/modules/cross_validation.html</p>
<p>https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a</p>
<p>https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py</p></div>

        <div>

    </body>

</html>

